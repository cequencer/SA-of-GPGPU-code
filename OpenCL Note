
*SCHEDULE
week 1 wcet
week 2 sa
week 3 wcet+sa
week 4 gpu/opencl
week 5 cache
week 6 
week 7

*WCET

*STATIC ANALYZE

*WCET + STATIC ANALYZE



*OPENCL PLATFORM/CONTEXT/DEVICE
A platform is a specific OpenCL implementation, for instance AMD APP, NVIDIA or Intel OpenCL.
A context is a platform with a set of available devices for that platform.
And the devices are the actual processors (CPU, GPU etc.) that perform calculations.

So if you use the Intel platform, a valid context with this platform would include a CPU device. While if you use the NVIDIA platform, a valid context would include an NVIDIA GPU device.

*OPENCL RUNTIME
    CL       GPU(NVIDIA CUDA-ENABLED)
   thread            core
thread block streaming multiprocessor
 kernel grid         GPU
 
 Execution model
 kernel
 program:analogous to a dynamic library for gpgpu
 application queue kernel execution instance

GPU devices use the Single Instruction Multiple Threads (SIMT) execution model. In the
model, threads are grouped into warps of constant size.



one contex may have multiple devices
 
 choose the dimension 'best' for the algorithm(1,2,or3)
 
 
 
 
 
*OPTIMIZING OPENCL CODE
uchar->uchar_16->uchar_16_vload
 
 
*CACHE MISS
Compulsory misses：也稱為cold start misses，第一次存取未曾在cache內的block而發生的cache miss。
Capacity misses：因為在程式執行期間，cache無法包含所有需要的block而產生的cache miss。發生在一個block被取代後，稍後卻又需要用到。
Conflict misses：發生在set-associative或direct-mapped caches，當多個blocks競爭相同的set。通常也稱作collision misses。
 
Memory hierarchy design challenges
Design change      Effect on miss rate   Possible negative performance effect
增加cache size     降低capacity misses   可能增加access time 
增加associativity  降低conflict misses   可能增加access time
增加block size     降低compulsory misses 增加miss penalty

scratch pad memory
no 'tag' needed, easy for static analysing

NVIDIA's 8800 GPU running under CUDA provides 16 KB of scratchpad (NVIDIA calls it Shared Memory) per thread-bundle when being used for GPGPU tasks. 

mem_fence does not cause execution to stop at that point, only that memory operations will not get reordered around the fence instruction. A barrier gaurantees that all work-items reach that point before any work-item moves to the next instruction



Memory And Cache Architecture
http://supercomputingblog.com/cuda/cuda-memory-and-cache-architecture/










reference
https://software.intel.com/en-us/videos/optimizing-simple-opencl-kernels-modulate-kernel-optimization








https://software.intel.com/en-us/articles/opencl-tutorials
http://www.cmsoft.com.br/opencl-tutorial/
http://dhruba.name/2012/08/21/opencl-cookbook-series-reference/
https://www.fixstars.com/en/opencl/book/OpenCLProgrammingBook/first-opencl-program/
http://developer.amd.com/tools-and-sdks/opencl-zone/opencl-resources/programming-in-opencl/
http://www2.kimicat.com/gpu%E7%9A%84%E7%A1%AC%E9%AB%94%E6%9E%B6%E6%A7%8B
http://www.cc.ntu.edu.tw/chinese/epaper/0012/20100320_1205.htm
